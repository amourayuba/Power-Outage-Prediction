{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec023bbb-e26f-4ae5-b6c0-aa312e3b7fbc",
   "metadata": {},
   "source": [
    "# Cleaning and merging weather and power data\n",
    "This file contains general purpose scripts for downloading, cleaning, and merging weather and power data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e9339c-7312-4ada-a779-b6839e8f1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ftplib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd4ee1-9b10-4f03-b118-84e714ac05e5",
   "metadata": {},
   "source": [
    "## Automatic downloading and reading of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55206573-6171-4de0-8e03-3717ed274ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = {'hail':'hail', 'storm_structure':'structure', 'tornados':'tvs', 'lightning':'nldn-tiles', 'mesocyclone':'mda'}\n",
    "for event in event_types:\n",
    "    path = '../../weather_data/'+event\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b3e854-67ad-44a2-937d-a2c86ad0991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the server\n",
    "ftp = ftplib.FTP('ftp.ncdc.noaa.gov', timeout=30) #pass the url without protocol\n",
    "ftp.login() #pass credentials if anonymous access is not allowed\n",
    "\n",
    "# switch to the directory containing the data\n",
    "ftp.cwd('/pub/data/swdi/database-csv/v2/')\n",
    "ftp.pwd()\n",
    "\n",
    "httpurl = 'https://www.ncei.noaa.gov/pub/data/swdi/database-csv/v2/'\n",
    "# get the list of files in this ftp dir\n",
    "all_files= ftp.nlst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebd25ab-9285-49f9-b34c-e41214431795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weather(year, event_type):\n",
    "    print(f'Downloading {event_type} file for {year}.')\n",
    "    event_name = event_types[event_type]\n",
    "    pattern = event_name+\"-\"+str(year)\n",
    "    file_name = [fname for fname in all_files if pattern in fname]\n",
    "    if len(file_name) == 0:\n",
    "        print(\"No file in that year for that event type\" )\n",
    "        return \n",
    "    file_name = file_name[0]\n",
    "    print(\"Considering file \", file_name)\n",
    "    if os.path.exists('../../weather_data/{}/{}'.format(event_type, file_name)):\n",
    "        print(\"file already exists\")\n",
    "        return \n",
    "    query_parameters = {\"downloadformat\": \"csv\"}\n",
    "    print(\"Getting the response from the URL .....\")\n",
    "    response = requests.get(httpurl+file_name, params=query_parameters)\n",
    "    if response.ok:\n",
    "        print(\"Downloaded succesfully\")\n",
    "    with open(r'../../weather_data/{}/{}'.format(event_type, file_name), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print('Saved in folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1085542-bf79-4235-89b9-27c775290d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weather(year, event_type):\n",
    "    download_weather(year,event_type)\n",
    "    files = os.listdir('../../weather_data/'+event_type)\n",
    "    file_name = [fname for fname in files if str(year) in fname]\n",
    "    if len(file_name) == 0:\n",
    "        raise Exception(f\"No file for event type {event_type} in year {year}\") \n",
    "    if len(file_name) > 1:\n",
    "        raise Exception(f\"Multiple files for event type {event_type} in year {year}\")\n",
    "    if event_type == 'lightning' or event_type == 'tornado':\n",
    "        return pd.read_csv(r'../../weather_data/'+event_type+'/'\n",
    "                  + file_name[0], skiprows=2, parse_dates=['#ZDAY'])\n",
    "    return pd.read_csv(r'../../weather_data/'+event_type+'/'\n",
    "                  + file_name[0], skiprows=2, parse_dates=['#ZTIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fedfa-494d-4ead-8c74-b078d1251d25",
   "metadata": {},
   "source": [
    "## Cleaning power data\n",
    "We clean the power data as follows.\n",
    "1. Convert 'Date Event Began' column to datetime format.\n",
    "2. Keep only the rows where power event type involves 'Severe Weather'.\n",
    "3. Drop all columns except Date Event Began and Area Affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3338c728-a58f-4e70-a602-12c71cee296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_power(power):\n",
    "    power['Date Event Began'] = pd.to_datetime(power['Date Event Began'], format='%m/%d/%Y')\n",
    "    power = power[power['Event Type'].str.contains(r'Severe Weather', regex=True)]\n",
    "    return power.drop(columns=['Month', 'Time Event Began', 'Date of Restoration', 'Time of Restoration', \n",
    "                         'NERC Region', 'Alert Criteria', 'Event Type', 'Demand Loss (MW)', 'Number of Customers Affected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81680192-a3bb-469d-bfe1-81c79c350866",
   "metadata": {},
   "source": [
    "## Cleaning weather data\n",
    "We clean weather data as follows.\n",
    "1. Convert '#ZDAY' or '#ZTIME' column to 'DATE' column in datetime format, and add a separate 'MONTH' column as well.\n",
    "2. Split into groups by 'DATE', 'WSR_ID', and 'CELL_ID', then get the mean of 'LAT' and 'LON' in each group, and the max for each remaining attribute in each group. (For lightning data, this step is not necessary.)\n",
    "3. Reverse geosearch using 'LAT' and 'LON' to get 'COUNTY' and 'STATE' columns.\n",
    "4. Drop the rows where no county info is found (indicating an event outside of the US)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ac61ba-4b8a-440f-af9b-fd31c364f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(weather):\n",
    "    weather['DATE'] = pd.to_datetime(weather['#ZTIME']).dt.normalize()\n",
    "    weather['MONTH'] = weather['DATE'].dt.month\n",
    "    return weather.drop(columns=['#ZTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ebb388b-1c29-4cfe-80ab-e6368f536b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_groups(weather, attributes):\n",
    "    groups = weather.groupby(['DATE', 'MONTH', 'WSR_ID', 'CELL_ID'], as_index=False)\n",
    "    return groups.agg(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9486f20-d1d3-4aa6-bd42-b4e7d598baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_and_state(weather):\n",
    "    addresses = rg.search(list(zip(weather['LAT'], weather['LON'])))\n",
    "    weather['COUNTY'] = [x['admin2'] for x in addresses]\n",
    "    weather['STATE'] = [x['admin1'] for x in addresses]\n",
    "    return weather[weather['COUNTY'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc52b2fc-ca92-481a-885d-64ed4c2330ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lightning(lightning):\n",
    "    lightning.rename(columns={'#ZDAY' : '#ZTIME', 'CENTERLAT': 'LAT', 'CENTERLON': 'LON'}, inplace=True)\n",
    "    lightning_dates = get_date(lightning)\n",
    "    return get_county_and_state(lightning_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5555aa2d-2f38-461c-88f4-741fbd8a4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tornado(tornados):\n",
    "    tornados_dates = get_date(tornados)\n",
    "    tornados_aggregate = aggregate_groups(tornados_dates, {'LAT':'mean',\n",
    "                                                           'LON':'mean',\n",
    "                                                           'AVGDV':'max', 'LLDV':'max', 'MXDV':'max',\n",
    "                                                           'MXDV_HEIGHT':'max', 'DEPTH':'max',\n",
    "                                                           'MAX_SHEAR':'max', 'MAX_SHEAR_HEIGHT':'max'})\n",
    "    tor_agg = get_county_and_state(tornados_aggregate)\n",
    "    return tor_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14da3c66-da23-49ef-b0ec-3a5b1d198819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hail(hail):\n",
    "    hail = hail[hail.SEVPROB>0]\n",
    "    hail_dates = get_date(hail)\n",
    "    hail_aggregate = aggregate_groups(hail_dates, {'LAT':'mean', \n",
    "                                                   'LON':'mean',\n",
    "                                                   'SEVPROB':'max', 'PROB':'max', 'MAXSIZE':'max'})\n",
    "    hail_agg = get_county_and_state(hail_aggregate)\n",
    "    return hail_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00d86171-2fbe-4e6f-b325-37f34ffda8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_meso(meso):\n",
    "    meso_dates = get_date(meso)\n",
    "    meso_aggregate = aggregate_groups(meso_dates, {'LAT':'mean', \n",
    "                                                  'LON':'mean', \n",
    "                                                  'STR_RANK':'max', 'LL_ROT_VEL':'max', \n",
    "                                                  'LL_DV':'max', 'LL_BASE':'max', 'DEPTH_KFT':'max', \n",
    "                                                  'DPTH_STMRL':'max', 'MAX_RV_KFT':'max', 'MAX_RV_KTS':'max', \n",
    "                                                  'TVS':'max', 'MSI':'max'})\n",
    "    meso_agg = get_county_and_state(meso_aggregate)\n",
    "    return meso_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2a4a558-8cfc-4843-8200-aee8c2bc3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_storm(storm):\n",
    "    storm_dates = get_date(storm)\n",
    "    storm_aggregate = aggregate_groups(storm_dates, {'LAT':'mean',\n",
    "                                                     'LON':'mean',\n",
    "                                                     'MAX_REFLECT':'max', 'VIL':'max', 'HEIGHT':'max'})\n",
    "    storm_agg = get_county_and_state(storm_aggregate)\n",
    "    return storm_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec52a67-86b2-46f4-86b6-ef3c6a7201be",
   "metadata": {},
   "source": [
    "## Merging\n",
    "We merge cleaned weather data with cleaned power data by adding a column specifying whether or not the recorded weather event resulted in a power outage in the same area on the same date. 'In the same area' is explained in the in_area function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbd012-673c-44b0-b0aa-0ea24ab2fff4",
   "metadata": {},
   "source": [
    "### Checking if weather and power event are in same county, or state if no county info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc180bde-7225-4a82-975d-4a9dff26525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all US states\n",
    "counties = pd.read_csv(\"../../extras/uscounties.csv\", index_col=0)\n",
    "counties['county'] = counties['county'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a562ebfe-65e3-40d1-b03a-f93b338dd977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_area(county, state, area_affected):\n",
    "    \"\"\"\n",
    "    input:\n",
    "\n",
    "    county, state: the county and state of the weather event\n",
    "    area_affected: the area affected by the power outage(a string listing states and possibly counties)\n",
    "    has_county_info: whether area_affected\n",
    "\n",
    "    output: True if either state and county are both in area_affected, or\n",
    "            False if state is in area_affected and there is no county info for area_affected\n",
    "    \"\"\"\n",
    "    if not county or not state or not area_affected:\n",
    "        raise Exception(f\"Invalid (null) input. county: {county}, state: {state}, area_affected: {area_affected}\")\n",
    "\n",
    "    # adding a colon to state ensures that it's matched exactly to a state in area_affected\n",
    "    # (rather than a county whose name is a state)\n",
    "    stateC = ''.join([state,':'])\n",
    "\n",
    "    # has_county_info is True if area_affected includes a county, false otherwise\n",
    "    has_county_info = any(cty in area_affected for cty in counties[counties['state'] == state]['county'])\n",
    "    \n",
    "    return stateC in area_affected and (county in area_affected or not has_county_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8e9c4-a947-4ae5-9134-810ec5985bfb",
   "metadata": {},
   "source": [
    "### Merging weather and power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0ba5a53-f99c-437c-8901-d019fac7314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weather_power(weather, power, light=True):\n",
    "    \"\"\"\n",
    "    Merge weather and power data.\n",
    "\n",
    "    Assumes that the input data are already cleaned.\n",
    "    \"\"\"\n",
    "    merged = pd.merge(weather, power, how='left', left_on='DATE', right_on='Date Event Began', indicator=True)\n",
    "    merged['POWER_OUTAGE'] = merged.apply(lambda row: (row['_merge'] == 'both') and in_area(str(row['COUNTY']),\n",
    "                                                                                            str(row['STATE']),\n",
    "                                                                                            str(row['Area Affected'])),\n",
    "                                          axis = 'columns')\n",
    "    if light:\n",
    "        try:\n",
    "            return merged.drop(columns=['Date Event Began', 'Area Affected', '_merge', 'WSR_ID', 'CELL_ID'])\n",
    "        except KeyError:\n",
    "            return merged.drop(columns=['Date Event Began', 'Area Affected', '_merge'])\n",
    "    return merged.drop(columns=['Date Event Began', 'Area Affected', '_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e82726a0-36f5-4e2c-8854-dcd1d6d009c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(year, event_type):\n",
    "    weather = read_weather(year,event_type)\n",
    "    if event_type == 'hail':\n",
    "        weather = clean_hail(weather)\n",
    "    elif event_type == 'storm_structure':\n",
    "        weather = clean_storm(weather)\n",
    "    elif event_type == 'tornados':\n",
    "        weather = clean_tornado(weather)\n",
    "    elif event_type == 'lightning':\n",
    "        weather = clean_lightning(weather)\n",
    "    elif event_type == 'mesocyclone':\n",
    "        weather = clean_meso(weather)\n",
    "    else:\n",
    "        raise Exception(f'Invalid event type: {event_type}, must be one of {event_types.keys()}')\n",
    "    power= pd.read_excel('../../power_data/' + str(year) + '_Annual_Summary.xls', skiprows=1)\n",
    "    power = clean_power(power)\n",
    "    return merge_weather_power(weather,power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce6c2f0b-d807-44bc-8d97-49614d84da79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading hail file for 2015.\n",
      "Considering file  hail-2015.csv.gz\n",
      "file already exists\n",
      "Loading formatted geocoded file...\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2016.\n",
      "Considering file  hail-2016.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2017.\n",
      "Considering file  hail-2017.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (69316) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2018.\n",
      "Considering file  hail-2018.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (86212) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2019.\n",
      "Considering file  hail-2019.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (96452) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2020.\n",
      "Considering file  hail-2020.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (120004) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2021.\n",
      "Considering file  hail-2021.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (133828) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2022.\n",
      "Considering file  hail-2022.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (134340) not 512 + multiple of sector size (512)\n",
      "Downloading hail file for 2023.\n",
      "Considering file  hail-2023.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (117444) not 512 + multiple of sector size (512)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Month'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(year, event_type)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid event type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_types\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m power\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../power_data/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(year) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Annual_Summary.xls\u001b[39m\u001b[38;5;124m'\u001b[39m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m power \u001b[38;5;241m=\u001b[39m \u001b[43mclean_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge_weather_power(weather,power)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mclean_power\u001b[0;34m(power)\u001b[0m\n\u001b[1;32m      2\u001b[0m power[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate Event Began\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(power[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate Event Began\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m power \u001b[38;5;241m=\u001b[39m power[power[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent Type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSevere Weather\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime Event Began\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate of Restoration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime of Restoration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNERC Region\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlert Criteria\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEvent Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDemand Loss (MW)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNumber of Customers Affected\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/core/frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/core/generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/core/generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Month'] not found in axis\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for year in range(2015,2023):\n",
    "    merged_hail = merge(year,'hail')\n",
    "    merged_hail.to_csv(f'../../merged/merged_hail_{year}.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff7ff90d-6a14-4988-b94d-5e2219e151f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading storm_structure file for 2020.\n",
      "Considering file  structure-2020.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (120004) not 512 + multiple of sector size (512)\n",
      "CPU times: user 9min 25s, sys: 8.51 s, total: 9min 34s\n",
      "Wall time: 10min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merged_storm_2020 = merge(2020,'storm_structure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d513e3b-849b-4ba6-8fcf-14b48d2cb1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>MAX_REFLECT</th>\n",
       "      <th>VIL</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>POWER_OUTAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>45.531415</td>\n",
       "      <td>-98.985618</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Edmunds County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>45.057635</td>\n",
       "      <td>-98.531035</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Spink County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>45.122425</td>\n",
       "      <td>-97.951135</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Clark County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>45.072180</td>\n",
       "      <td>-97.959420</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Clark County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>24.273560</td>\n",
       "      <td>-80.413000</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Monroe County</td>\n",
       "      <td>Florida</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  MONTH        LAT        LON  MAX_REFLECT  VIL  HEIGHT  \\\n",
       "0 2020-01-01      1  45.531415 -98.985618           38    1     3.2   \n",
       "1 2020-01-01      1  45.057635 -98.531035           38    0     2.9   \n",
       "2 2020-01-01      1  45.122425 -97.951135           40    1     2.1   \n",
       "3 2020-01-01      1  45.072180 -97.959420           35    0     2.2   \n",
       "4 2020-01-01      1  24.273560 -80.413000           47    6     8.8   \n",
       "\n",
       "           COUNTY         STATE  POWER_OUTAGE  \n",
       "0  Edmunds County  South Dakota         False  \n",
       "1    Spink County  South Dakota         False  \n",
       "2    Clark County  South Dakota         False  \n",
       "3    Clark County  South Dakota         False  \n",
       "4   Monroe County       Florida         False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_storm_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8eb058a9-f187-4640-9afb-4d584a9652cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tornados file for 2015.\n",
      "Considering file  tvs-2015.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2016.\n",
      "Considering file  tvs-2016.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2017.\n",
      "Considering file  tvs-2017.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (69316) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2018.\n",
      "Considering file  tvs-2018.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (86212) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2019.\n",
      "Considering file  tvs-2019.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (96452) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2020.\n",
      "Considering file  tvs-2020.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (120004) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2021.\n",
      "Considering file  tvs-2021.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (133828) not 512 + multiple of sector size (512)\n",
      "Downloading tornados file for 2022.\n",
      "Considering file  tvs-2022.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (134340) not 512 + multiple of sector size (512)\n",
      "CPU times: user 46.5 s, sys: 928 ms, total: 47.5 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for year in range(2015,2023):\n",
    "    merged_tornado = merge(year,'tornados')\n",
    "    merged_tornado.to_csv(f'../../merged/merged_tornado_{year}.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a13d5f8f-1092-4b48-8778-04db0f20e0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mesocyclone file for 2015.\n",
      "Considering file  mda-2015.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2016.\n",
      "Considering file  mda-2016.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2017.\n",
      "Considering file  mda-2017.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (69316) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2018.\n",
      "Considering file  mda-2018.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (86212) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2019.\n",
      "Considering file  mda-2019.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (96452) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2020.\n",
      "Considering file  mda-2020.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (120004) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2021.\n",
      "Considering file  mda-2021.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (133828) not 512 + multiple of sector size (512)\n",
      "Downloading mesocyclone file for 2022.\n",
      "Considering file  mda-2022.csv.gz\n",
      "Getting the response from the URL .....\n",
      "Downloaded succesfully\n",
      "Saved in folder\n",
      "WARNING *** file size (134340) not 512 + multiple of sector size (512)\n",
      "CPU times: user 14min 27s, sys: 5.92 s, total: 14min 33s\n",
      "Wall time: 14min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for year in range(2015,2023):\n",
    "    merged_meso = merge(year,'mesocyclone')\n",
    "    merged_meso.to_csv(f'../../merged/merged_meso_{year}.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14f3dd2f-c7e4-42ae-b6d5-70fa69795144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading lightning file for 2015.\n",
      "Considering file  nldn-tiles-2015.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2016.\n",
      "Considering file  nldn-tiles-2016.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (73412) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2017.\n",
      "Considering file  nldn-tiles-2017.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (69316) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2018.\n",
      "Considering file  nldn-tiles-2018.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (86212) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2019.\n",
      "Considering file  nldn-tiles-2019.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (96452) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2020.\n",
      "Considering file  nldn-tiles-2020.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (120004) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2021.\n",
      "Considering file  nldn-tiles-2021.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (133828) not 512 + multiple of sector size (512)\n",
      "Downloading lightning file for 2022.\n",
      "Considering file  nldn-tiles-2022.csv.gz\n",
      "file already exists\n",
      "WARNING *** file size (134340) not 512 + multiple of sector size (512)\n",
      "CPU times: user 30min 8s, sys: 11.4 s, total: 30min 19s\n",
      "Wall time: 30min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for year in range(2015,2023):\n",
    "    merged_lightning = merge(year,'lightning')\n",
    "    merged_lightning.to_csv(f'../../merged/merged_lightning_{year}.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ff409-8ee3-492f-94af-e33062bdbdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
